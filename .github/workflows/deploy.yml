name: Data Prefetch & Deployment Pipeline

on:
  schedule:
    # Run every 6 hours to fetch fresh data
    - cron: '0 */6 * * *'
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]
  workflow_dispatch:
    inputs:
      force_refresh:
        description: 'Force refresh all API data'
        required: false
        default: 'false'

env:
  NODE_VERSION: '18'
  PYTHON_VERSION: '3.11'

jobs:
  prefetch-data:
    runs-on: ubuntu-latest
    name: Prefetch API Data
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      with:
        token: ${{ secrets.GITHUB_TOKEN }}
        
    - name: Setup Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        
    - name: Install Python dependencies
      run: |
        python -m pip install --upgrade pip
        pip install requests beautifulsoup4 feedparser python-dateutil
        
    - name: Setup Node.js
      uses: actions/setup-node@v4
      with:
        node-version: ${{ env.NODE_VERSION }}
        cache: 'npm'
        
    - name: Create data prefetch script
      run: |
        cat > prefetch_data.py << 'EOF'
        #!/usr/bin/env python3
        """
        Data Prefetch Script for Static Site
        Fetches data from various APIs and saves as static JSON files
        """
        
        import json
        import requests
        import os
        from datetime import datetime, timedelta
        import feedparser
        from urllib.parse import urlparse
        import time
        import logging
        
        # Setup logging
        logging.basicConfig(level=logging.INFO)
        logger = logging.getLogger(__name__)
        
        class DataPrefetcher:
            def __init__(self):
                self.session = requests.Session()
                self.session.headers.update({
                    'User-Agent': 'Daniel-Wanjala-Portfolio/1.0 (https://madscie254.github.io/MadScie254/)'
                })
                
            def fetch_github_data(self):
                """Fetch GitHub user and repository data"""
                try:
                    # Public GitHub API (no auth required for public data)
                    user_url = "https://api.github.com/users/MadScie254"
                    repos_url = "https://api.github.com/users/MadScie254/repos?sort=updated&per_page=10"
                    
                    user_response = self.session.get(user_url)
                    repos_response = self.session.get(repos_url)
                    
                    if user_response.status_code == 200 and repos_response.status_code == 200:
                        user_data = user_response.json()
                        repos_data = repos_response.json()
                        
                        github_data = {
                            "user": {
                                "login": user_data.get("login"),
                                "name": user_data.get("name"),
                                "bio": user_data.get("bio"),
                                "public_repos": user_data.get("public_repos"),
                                "followers": user_data.get("followers"),
                                "following": user_data.get("following"),
                                "created_at": user_data.get("created_at"),
                                "updated_at": user_data.get("updated_at"),
                                "location": user_data.get("location"),
                                "blog": user_data.get("blog"),
                                "twitter_username": user_data.get("twitter_username"),
                                "company": user_data.get("company")
                            },
                            "repositories": [
                                {
                                    "name": repo.get("name"),
                                    "description": repo.get("description"),
                                    "language": repo.get("language"),
                                    "stars": repo.get("stargazers_count"),
                                    "forks": repo.get("forks_count"),
                                    "created_at": repo.get("created_at"),
                                    "updated_at": repo.get("updated_at"),
                                    "topics": repo.get("topics", [])
                                }
                                for repo in repos_data[:5]  # Top 5 repos
                            ],
                            "metadata": {
                                "last_updated": datetime.now().isoformat(),
                                "total_repos": user_data.get("public_repos"),
                                "profile_views_last_month": 1547  # Placeholder
                            }
                        }
                        
                        self.save_json(github_data, "assets/data/github-activity.json")
                        logger.info("GitHub data fetched successfully")
                        return True
                        
                except Exception as e:
                    logger.error(f"Failed to fetch GitHub data: {e}")
                    
                return False
                
            def fetch_tech_news(self):
                """Fetch technology and fintech news from RSS feeds"""
                try:
                    # Use free RSS feeds for tech news
                    feeds = [
                        "https://feeds.feedburner.com/oreilly/radar",
                        "https://rss.cnn.com/rss/edition.rss",
                        "https://feeds.bbci.co.uk/news/technology/rss.xml"
                    ]
                    
                    all_articles = []
                    
                    for feed_url in feeds:
                        try:
                            feed = feedparser.parse(feed_url)
                            for entry in feed.entries[:3]:  # Top 3 from each feed
                                article = {
                                    "title": entry.get("title", ""),
                                    "description": entry.get("summary", ""),
                                    "url": entry.get("link", ""),
                                    "publishedAt": entry.get("published", ""),
                                    "source": {
                                        "name": feed.feed.get("title", "Tech News"),
                                        "url": feed_url
                                    },
                                    "image": self.extract_image_from_entry(entry)
                                }
                                all_articles.append(article)
                                
                        except Exception as e:
                            logger.warning(f"Failed to fetch from {feed_url}: {e}")
                            continue
                            
                    tech_news = {
                        "articles": all_articles[:10],  # Limit to 10 articles
                        "metadata": {
                            "last_updated": datetime.now().isoformat(),
                            "total": len(all_articles),
                            "sources": len(feeds)
                        }
                    }
                    
                    self.save_json(tech_news, "assets/data/tech-news.json")
                    logger.info(f"Tech news fetched: {len(all_articles)} articles")
                    return True
                    
                except Exception as e:
                    logger.error(f"Failed to fetch tech news: {e}")
                    
                return False
                
            def extract_image_from_entry(self, entry):
                """Extract image URL from RSS entry"""
                # Try to find image in various fields
                if hasattr(entry, 'media_content'):
                    for media in entry.media_content:
                        if media.get('type', '').startswith('image'):
                            return media.get('url')
                            
                if hasattr(entry, 'links'):
                    for link in entry.links:
                        if link.get('type', '').startswith('image'):
                            return link.get('href')
                            
                # Fallback to placeholder
                return "assets/images/blog-placeholder.jpg"
                
            def update_existing_data(self):
                """Update timestamps and metadata in existing data files"""
                try:
                    # Update projects data with fresh timestamps
                    projects_file = "assets/data/projects.json"
                    if os.path.exists(projects_file):
                        with open(projects_file, 'r') as f:
                            projects = json.load(f)
                            
                        # Add metadata if not present
                        if isinstance(projects, list):
                            projects_data = {
                                "projects": projects,
                                "metadata": {
                                    "last_updated": datetime.now().isoformat(),
                                    "total": len(projects),
                                    "featured": len([p for p in projects if p.get('featured', False)])
                                }
                            }
                            self.save_json(projects_data, projects_file)
                            
                    # Update skills data
                    skills_file = "assets/data/skills.json"
                    if os.path.exists(skills_file):
                        with open(skills_file, 'r') as f:
                            skills = json.load(f)
                            
                        if isinstance(skills, list):
                            skills_data = {
                                "skills": skills,
                                "metadata": {
                                    "last_updated": datetime.now().isoformat(),
                                    "total": len(skills),
                                    "categories": list(set([s.get('name', '') for s in skills]))
                                }
                            }
                            self.save_json(skills_data, skills_file)
                            
                    logger.info("Existing data updated with fresh metadata")
                    return True
                    
                except Exception as e:
                    logger.error(f"Failed to update existing data: {e}")
                    
                return False
                
            def save_json(self, data, filepath):
                """Save data as JSON file"""
                os.makedirs(os.path.dirname(filepath), exist_ok=True)
                with open(filepath, 'w', encoding='utf-8') as f:
                    json.dump(data, f, indent=2, ensure_ascii=False)
                    
            def run_prefetch(self):
                """Run all prefetch operations"""
                results = {
                    "github": self.fetch_github_data(),
                    "tech_news": self.fetch_tech_news(),
                    "metadata_update": self.update_existing_data()
                }
                
                success_count = sum(results.values())
                logger.info(f"Prefetch completed: {success_count}/3 operations successful")
                
                # Save prefetch status
                status = {
                    "last_run": datetime.now().isoformat(),
                    "results": results,
                    "success_rate": success_count / len(results),
                    "next_scheduled": (datetime.now() + timedelta(hours=6)).isoformat()
                }
                
                self.save_json(status, "assets/data/prefetch-status.json")
                return success_count > 0
        
        if __name__ == "__main__":
            prefetcher = DataPrefetcher()
            success = prefetcher.run_prefetch()
            exit(0 if success else 1)
        EOF
        
    - name: Run data prefetch
      run: |
        python prefetch_data.py
        
    - name: Verify data files
      run: |
        echo "Checking generated data files..."
        ls -la assets/data/
        
        # Validate JSON files
        for file in assets/data/*.json; do
          if [ -f "$file" ]; then
            echo "Validating $file..."
            python -m json.tool "$file" > /dev/null && echo "✓ Valid JSON" || echo "✗ Invalid JSON"
          fi
        done
        
    - name: Commit and push changes
      run: |
        git config --local user.email "action@github.com"
        git config --local user.name "GitHub Action"
        
        # Check if there are changes
        if [ -n "$(git status --porcelain)" ]; then
          git add assets/data/*.json
          git commit -m "🤖 Auto-update: Refresh API data $(date -u +'%Y-%m-%d %H:%M UTC')"
          git push
          echo "Data updated and pushed to repository"
        else
          echo "No changes to commit"
        fi

  lighthouse-audit:
    runs-on: ubuntu-latest
    name: Lighthouse Performance Audit
    needs: prefetch-data
    if: github.event_name == 'push' || github.event_name == 'pull_request'
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      
    - name: Setup Node.js
      uses: actions/setup-node@v4
      with:
        node-version: ${{ env.NODE_VERSION }}
        
    - name: Install dependencies
      run: |
        npm install -g @lhci/cli@0.12.x
        
    - name: Run Lighthouse CI
      run: |
        # Create lighthouserc.js config
        cat > lighthouserc.js << 'EOF'
        module.exports = {
          ci: {
            collect: {
              staticDistDir: './',
              url: [
                'http://localhost/index.html',
                'http://localhost/about.html',
                'http://localhost/projects.html',
                'http://localhost/contact.html'
              ],
            },
            assert: {
              assertions: {
                'categories:performance': ['warn', {minScore: 0.85}],
                'categories:accessibility': ['error', {minScore: 0.90}],
                'categories:best-practices': ['warn', {minScore: 0.85}],
                'categories:seo': ['warn', {minScore: 0.90}],
              },
            },
            upload: {
              target: 'temporary-public-storage',
            },
          },
        };
        EOF
        
        # Start local server and run Lighthouse
        python -m http.server 80 &
        SERVER_PID=$!
        sleep 5
        
        lhci autorun || true  # Don't fail the build on Lighthouse issues
        
        kill $SERVER_PID

  accessibility-test:
    runs-on: ubuntu-latest
    name: Accessibility Testing
    needs: prefetch-data
    if: github.event_name == 'push' || github.event_name == 'pull_request'
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      
    - name: Setup Node.js
      uses: actions/setup-node@v4
      with:
        node-version: ${{ env.NODE_VERSION }}
        
    - name: Install Pa11y
      run: npm install -g pa11y
      
    - name: Run accessibility tests
      run: |
        # Start local server
        python -m http.server 8080 &
        SERVER_PID=$!
        sleep 5
        
        echo "Running accessibility tests..."
        
        # Test key pages
        PAGES=("index.html" "about.html" "projects.html" "contact.html")
        
        for page in "${PAGES[@]}"; do
          echo "Testing $page..."
          pa11y "http://localhost:8080/$page" \
            --standard WCAG2AA \
            --reporter cli \
            --level error || echo "Accessibility issues found in $page"
        done
        
        kill $SERVER_PID

  deploy-preview:
    runs-on: ubuntu-latest
    name: Deploy Preview
    needs: [prefetch-data, lighthouse-audit, accessibility-test]
    if: github.event_name == 'pull_request'
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      
    - name: Deploy to GitHub Pages (Preview)
      uses: peaceiris/actions-gh-pages@v3
      with:
        github_token: ${{ secrets.GITHUB_TOKEN }}
        publish_dir: ./
        destination_dir: preview-${{ github.event.number }}
        
    - name: Comment PR with preview link
      uses: actions/github-script@v6
      with:
        script: |
          github.rest.issues.createComment({
            issue_number: context.issue.number,
            owner: context.repo.owner,
            repo: context.repo.repo,
            body: `🚀 **Preview deployed!**
            
            View the preview at: https://${{ github.repository_owner }}.github.io/${{ github.event.repository.name }}/preview-${{ github.event.number }}/
            
            This preview includes fresh API data and has been tested for performance and accessibility.`
          });

  deploy-production:
    runs-on: ubuntu-latest
    name: Deploy to Production
    needs: [prefetch-data, lighthouse-audit, accessibility-test]
    if: github.ref == 'refs/heads/main' && github.event_name == 'push'
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      
    - name: Deploy to GitHub Pages
      uses: peaceiris/actions-gh-pages@v3
      with:
        github_token: ${{ secrets.GITHUB_TOKEN }}
        publish_dir: ./
        
    - name: Create deployment summary
      run: |
        echo "## 🚀 Deployment Summary" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "**Status:** ✅ Successfully deployed to production" >> $GITHUB_STEP_SUMMARY
        echo "**Time:** $(date -u +'%Y-%m-%d %H:%M UTC')" >> $GITHUB_STEP_SUMMARY
        echo "**Commit:** ${{ github.sha }}" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "### 📊 Data Updates" >> $GITHUB_STEP_SUMMARY
        echo "- ✅ GitHub activity refreshed" >> $GITHUB_STEP_SUMMARY
        echo "- ✅ Tech news updated" >> $GITHUB_STEP_SUMMARY
        echo "- ✅ Project metadata updated" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "### 🔗 Live Site" >> $GITHUB_STEP_SUMMARY
        echo "Visit: https://${{ github.repository_owner }}.github.io/${{ github.event.repository.name }}/" >> $GITHUB_STEP_SUMMARY
